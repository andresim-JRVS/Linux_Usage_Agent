## Introduction
This project monitors the CPU and memory usage across various machines. The usage statistics and machine information are stored within a PostgresSQL database. This allows users to get details on cluster usage and plan for future growth.

## Architecture and Design
![enter image description here](https://picasaweb.google.com/111162805281333903242/6706223186306464801#6706223184459835170 "Architecture Diagram")

As seen above, the Bash Agent scripts run on each machine within the cluster. The scripts will pass the information they gather to the PostgresSQL instance where it will be stored for further analysis.

The SQL tables themselves store quite a bit of information. There are two tables:
**host_info**

id: An id number associated with the computer
hostname: The name of the computer
cpu_number: Number of cores in the CPU
cpu_architecture: Whether the cpu is x86, x64, x32
cpu_model: Manufacturer name and model.
cpu_mhz: MHz CPU is set to
L2_cache: Size of the L2 cache
timestamp: Timestamp of when the information was gathered
total_mem: total memory on the computer.

**host_usage**

timestamp: Timestamp of when the usage statistic was taken
host_id: ID number referencing the host_info table, to show which computer has this usage.
memory_free: Shows free memory at the time script was run
cpu_idel: Shows Idle % of CPU
cpu_kernel: Shows % of CPU running kernel processes
disk_io: Shows current disk I/O
disak_available: Shows current available space on the disk

**host_info.sh**
This bash script will read in the connection details for the PostgresSQL instance and then collect the appropriate information from the machine the script runs on. It then puts all the information into a SQL query to insert the information into the host_info table in the database. The query is executed on the database using the connection details. Finally, the ID number generated by PostgresSQL is stored in a local file to keep track of how many machines have their information stored.

**host_usage.sh**
This script once again takes in connection details for the PostgresSQL database. This script then collects the current usage statistics of the machine it is running on. The script also gets the ID number of the machine it is running on by connecting to and querying the host_info table, getting the ID that matches with the current hostname. Once again it puts all the information into a SQL query. The query inserts the information into the host_usage table by connecting to the database using the connection details and executing the insert statement.

## Usage
**Initializing DB and Tables**
Prior to running the scripts, the database and schema must be created. init.sql contains all of the queries to create the database and tables. You can either execute the entire file on your preferred RDBMS or open the file and execute each query one at a time.

**Using host_info.sh**
After creating the database schema, the host_info.sh script will only need to be run once per machine. To run this script:
1. Open a terminal window
2. Navigate to the file location that contains the script.
3. Run the script by entering './host_info.sh' into the terminal without the single quotes.

**Using host_usage.sh**
The host_usage script is set to run automatically with crontab, however it can also be run manually. To run the script manually, follow the steps for host_usage.sh but replace the name of the script in step 3. To run the script automatically, follow the crontab Setup instructions below.

**crontab Setup**
To create a crontab job that runs the host_usage.sh script every minute do the following:
1. Open a terminal window.
2. Enter 'crontab -e' to edit crontab jobs
3. Enter '***** bash /*path to file*/host_usage.sh *database connection details* > /tmp/host_usage.log' replacing the path to file and database connection details with the appropriate info.
4.  Verify the crontab job was created successfully by entering 'crontab -ls'

## Improvements
1. Use an SSL connection to connect with PSQL in order to improve security. Although this is designed for a single network, improving security without any cost to users is generally a good thing and could allow this script to be run on more expansive networks.

2. Send alerts if usage is excessively high or script repeatedly fails. This would require a separate script that sends some alert to be triggered when usage exceeds a certain % or if the scripts repeatedly fail and would allow for closer, real-time monitoring.

3. Get more detailed usage information. Getting the number of processes being run and the % of CPU spent on user processes would allow for more in-depth usage analysis.


